--- openmpi/ompi/mca/io/romio/romio/adio/ad_plfs/ad_plfs.c	1969-12-31 17:00:00.000000000 -0700
+++ openmpi-plfs-patch/ompi/mca/io/romio/romio/adio/ad_plfs/ad_plfs.c	2014-10-15 11:15:40.489480000 -0600
@@ -0,0 +1,357 @@
+/* -*- Mode: C; c-basic-offset:4 ; -*- */
+/*
+ *   $Id: ad_plfs.c,v 1.1 2010/11/29 19:59:01 adamm Exp $
+ *
+ *   Copyright (C) 2001 University of Chicago.
+ *   See COPYRIGHT notice in top-level directory.
+ */
+
+#include "ad_plfs.h"
+
+#ifdef ROMIO_CRAY
+#include "../ad_cray/ad_cray.h"
+#endif /* ROMIO_CRAY */
+
+/* adioi.h has the ADIOI_Fns_struct define */
+#include "adioi.h"
+
+struct ADIOI_Fns_struct ADIO_PLFS_operations = {
+    ADIOI_PLFS_Open, /* Open */
+#ifndef ROMIO_OPENMPI_14x
+    /*
+     * The ROMIO that is in Open MPI 1.4.x is old and doesn't have
+     * the following entry or Feature
+     */
+    ADIOI_GEN_OpenColl,  /* OpenColl */
+#endif
+    ADIOI_PLFS_ReadContig, /* ReadContig */
+    ADIOI_PLFS_WriteContig, /* WriteContig */
+#ifdef ROMIO_CRAY
+    ADIOI_CRAY_ReadStridedColl, /* ReadStridedColl */
+    ADIOI_CRAY_WriteStridedColl, /* WriteStridedColl */
+#else
+    ADIOI_GEN_ReadStridedColl, /* ReadStridedColl */
+    ADIOI_GEN_WriteStridedColl, /* WriteStridedColl */
+#endif /* ROMIO_CRAY */
+    ADIOI_GEN_SeekIndividual, /* SeekIndividual */
+    ADIOI_PLFS_Fcntl, /* Fcntl */
+    ADIOI_PLFS_SetInfo, /* SetInfo */
+    ADIOI_GEN_ReadStrided, /* ReadStrided */
+    ADIOI_GEN_WriteStrided, /* WriteStrided */
+    ADIOI_PLFS_Close, /* Close */
+    ADIOI_FAKE_IreadContig, /* IreadContig */
+    ADIOI_FAKE_IwriteContig, /* IwriteContig */
+    ADIOI_FAKE_IODone, /* ReadDone */
+    ADIOI_FAKE_IODone, /* WriteDone */
+    ADIOI_FAKE_IOComplete, /* ReadComplete */
+    ADIOI_FAKE_IOComplete, /* WriteComplete */
+    ADIOI_FAKE_IreadStrided, /* IreadStrided */
+    ADIOI_FAKE_IwriteStrided, /* IwriteStrided */
+    ADIOI_PLFS_Flush, /* Flush */
+    ADIOI_PLFS_Resize, /* Resize */
+    ADIOI_PLFS_Delete, /* Delete */
+#ifndef ROMIO_OPENMPI_14x
+    /*
+     * The ROMIO that is in Open MPI 1.4.x is old and doesn't have
+     * the following entry or OpenColl
+     */
+     ADIOI_PLFS_Feature, /* Features */
+#endif
+};
+
+#ifdef ROMIO_CRAY
+/* --BEGIN CRAY ADDITION-- */
+
+/*
+ * If the PLFS library is not linked into the executable, these weak
+ * stubs will be called to issue an informative message and then abort.
+ */
+
+static void no_link_abort(void)
+{
+    FPRINTF(stderr,"A PLFS routine was called but the PLFS library "
+                   "is not linked into the program\n");
+    MPI_Abort(MPI_COMM_WORLD, __LINE__);
+}
+
+plfs_error_t plfs_close(Plfs_fd *,pid_t,uid_t,int open_flags,Plfs_close_opt *close_opt,int *)  __attribute__ ((weak));
+plfs_error_t plfs_close(Plfs_fd *fd,pid_t pid,uid_t uid,int open_flags,Plfs_close_opt *close_opt,int *num_ref)
+{
+    no_link_abort();
+    return PLFS_ENOSYS; /* never gets here */
+}
+
+plfs_error_t plfs_create( const char *path, mode_t mode, int flags, pid_t pid )  __attribute__ ((weak));
+plfs_error_t plfs_create( const char *path, mode_t mode, int flags, pid_t pid )
+{
+    no_link_abort();
+    return PLFS_ENOSYS; /* never gets here */
+}
+
+void plfs_debug( const char *format, ... )  __attribute__ ((weak));
+void plfs_debug( const char *format, ... )
+{
+    no_link_abort();
+}
+
+plfs_error_t plfs_expand_path(const char *logical,char **physical,void **pmountp, void **pbackp)  __attribute__ ((weak));
+plfs_error_t plfs_expand_path(const char *logical,char **physical, void **pmountp, void **pbackp)
+{
+    no_link_abort();
+    return PLFS_ENOSYS; /* never gets here */
+}
+
+plfs_error_t plfs_flatten_index( Plfs_fd *, const char *path )  __attribute__ ((weak));
+plfs_error_t plfs_flatten_index( Plfs_fd *fd, const char *path )
+{
+    no_link_abort();
+    return PLFS_ENOSYS; /* never gets here */
+}
+
+plfs_error_t plfs_getattr(Plfs_fd *, const char *path, struct stat *st, int size_only)  __attribute__ ((weak));
+plfs_error_t plfs_getattr(Plfs_fd *fd, const char *path, struct stat *st, int size_only)
+{
+    no_link_abort();
+    return PLFS_ENOSYS; /* never gets here */
+}
+
+size_t container_gethostdir_id(char *)  __attribute__ ((weak));
+size_t container_gethostdir_id(char *id)
+{
+    no_link_abort();
+    return 1; /* never gets here */
+}
+
+plfs_error_t plfs_gethostname(char **hname)  __attribute__ ((weak));
+plfs_error_t plfs_gethostname(char **hname)
+{
+    no_link_abort();
+    return PLFS_ENOSYS; /* never gets here */
+}
+
+plfs_error_t container_hostdir_rddir(void **index_stream,char *targets,
+                       int rank,char * top_level, void *pmount,
+                       void *pback, int *index_sz)  __attribute__ ((weak));
+plfs_error_t container_hostdir_rddir(void **index_stream,char *targets,
+                       int rank,char * top_level, void *pmount, void *pback,
+                       int *index_sz)
+{
+    no_link_abort();
+    return PLFS_ENOSYS; /* never gets here */
+}
+
+plfs_error_t container_hostdir_zero_rddir(void **entries,const char* path,int rank,
+                            void *pmount, void *pback, int *res_size)  __attribute__ ((weak));
+plfs_error_t container_hostdir_zero_rddir(void **entries,const char* path,int rank,
+                            void *pmount, void *pback, int *res_size)
+{
+    no_link_abort();
+    return PLFS_ENOSYS; /* never gets here */
+}
+
+plfs_error_t container_index_stream(Plfs_fd **pfd, char ** buffer, int *res_index_sz)  __attribute__ ((weak));
+plfs_error_t container_index_stream(Plfs_fd **pfd, char ** buffer, int *res_index_sz)
+{
+    no_link_abort();
+    return PLFS_ENOSYS; /* never gets here */
+}
+
+plfs_error_t container_merge_indexes(Plfs_fd **pfd, char *index_streams,
+                        int *index_sizes, int procs)  __attribute__ ((weak));
+plfs_error_t container_merge_indexes(Plfs_fd **pfd, char *index_streams,
+                        int *index_sizes, int procs)
+{
+    no_link_abort();
+    return PLFS_ENOSYS; /* never gets here */
+}
+
+plfs_error_t plfs_access(const char *path, int mask) __attribute__ ((weak));
+
+plfs_error_t plfs_access(const char *path, int mask){
+    no_link_abort();
+    return PLFS_ENOSYS;
+}
+
+plfs_error_t plfs_open( Plfs_fd **, const char *path,
+        int flags, pid_t pid, mode_t , Plfs_open_opt *open_opt)  __attribute__ ((weak));
+plfs_error_t plfs_open( Plfs_fd **fd, const char *path,
+        int flags, pid_t pid, mode_t mode, Plfs_open_opt *open_opt)
+{
+    no_link_abort();
+    return PLFS_ENOSYS; /* never gets here */
+}
+
+plfs_error_t container_parindex_read(int rank, int ranks_per_comm,void *index_files,
+        void **index_stream,char *top_level, int *res_index_size)  __attribute__ ((weak));
+plfs_error_t container_parindex_read(int rank, int ranks_per_comm,void *index_files,
+        void **index_stream,char *top_level, int *res_index_size)
+{
+    no_link_abort();
+    return PLFS_ENOSYS; /* never gets here */
+}
+
+int container_parindexread_merge(const char *path,char *index_streams,
+    int *index_sizes, int procs, void **index_stream)  __attribute__ ((weak));
+int container_parindexread_merge(const char *path,char *index_streams,
+    int *index_sizes, int procs, void **index_stream)
+{
+    no_link_abort();
+    return -1; /* never gets here */
+}
+
+plfs_error_t plfs_read( Plfs_fd *, char *buf, size_t size, off_t offset, ssize_t *bytes_read )
+    __attribute__ ((weak));
+plfs_error_t plfs_read( Plfs_fd *fd, char *buf, size_t size, off_t offset, ssize_t *bytes_read )
+{
+    no_link_abort();
+    return PLFS_ENOSYS; /* never gets here */
+}
+
+plfs_error_t plfs_sync( Plfs_fd *)  __attribute__ ((weak));
+plfs_error_t plfs_sync( Plfs_fd *fd)
+{
+    no_link_abort();
+    return PLFS_ENOSYS; /* never gets here */
+}
+
+plfs_error_t plfs_trunc( Plfs_fd *, const char *path, off_t, int open_file )
+    __attribute__ ((weak));
+plfs_error_t plfs_trunc( Plfs_fd *fd, const char *path, off_t off, int open_file )
+{
+    no_link_abort();
+    return PLFS_ENOSYS; /* never gets here */
+}
+
+plfs_error_t plfs_unlink( const char *path )  __attribute__ ((weak));
+plfs_error_t plfs_unlink( const char *path )
+{
+    no_link_abort();
+    return PLFS_ENOSYS; /* never gets here */
+}
+
+plfs_error_t plfs_write( Plfs_fd *, const char *, size_t, off_t, pid_t, ssize_t * )
+    __attribute__ ((weak));
+plfs_error_t plfs_write( Plfs_fd *fd,
+    const char *buf, size_t size, off_t off, pid_t pid, ssize_t *bytes_written)
+{
+    no_link_abort();
+    return PLFS_ENOSYS; /* never gets here */
+}
+
+plfs_error_t container_protect(const char *, pid_t)
+     __attribute__ ((weak));
+plfs_error_t container_protect(const char *path, pid_t pid)
+{
+    no_link_abort();
+    return PLFS_ENOSYS; /* never gets here */
+}
+
+plfs_error_t plfs_query( Plfs_fd *, size_t *, size_t *, size_t *, int *)
+     __attribute__ ((weak));
+plfs_error_t plfs_query( Plfs_fd *fd, size_t *writers, size_t *readers,
+                    size_t *bytes_written, int *lazy_stat)
+{
+    no_link_abort();
+    return PLFS_ENOSYS; /* never gets here */
+}
+
+plfs_filetype plfs_get_filetype(const char *)
+     __attribute__ ((weak));
+plfs_filetype plfs_get_filetype(const char *path)
+{
+    no_link_abort();
+    return -1; /* never gets here */
+}
+
+plfs_error_t container_num_host_dirs(int *, char *, void *, char *)
+    __attribute__ ((weak));
+plfs_error_t container_num_host_dirs(int *hostdir_count, char *target, void *vback, 
+                                     char *bm)
+{
+   no_link_abort();
+   return PLFS_ENOSYS; /* never gets here */
+}
+
+int plfs_error_to_errno(plfs_error_t)  
+    __attribute__ ((weak));
+int plfs_error_to_errno(plfs_error_t plfs_err)
+{
+   no_link_abort();
+   return -1; /* never gets here */
+}
+
+const char *strplfserr(plfs_error_t) 
+   __attribute__ ((weak));
+const char *strplfserr(plfs_error_t err)
+{
+   no_link_abort();
+   return NULL; /* never gets here */
+}
+
+/* --END CRAY ADDITION-- */
+#endif
+
+int plfs_protect_all(const char *file, MPI_Comm comm) {
+    int rank;
+    MPI_Comm_rank(comm,&rank);
+    plfs_error_t plfs_ret = container_protect(file,(pid_t)rank);
+    return -(plfs_error_to_errno(plfs_ret));
+}
+
+int ad_plfs_amode( int access_mode )
+{
+    int amode = 0; // O_META;
+    if (access_mode & ADIO_RDONLY) {
+        amode = amode | O_RDONLY;
+    }
+    if (access_mode & ADIO_WRONLY) {
+        amode = amode | O_WRONLY;
+    }
+    if (access_mode & ADIO_RDWR) {
+        amode = amode | O_RDWR;
+    }
+    if (access_mode & ADIO_EXCL) {
+        amode = amode | O_EXCL;
+    }
+    return amode;
+}
+
+
+
+int ad_plfs_hints(ADIO_File fd, int rank, char *hint)
+{
+    int hint_value=0,flag,resultlen,mpi_ret;
+    char *value;
+    char err_buffer[MPI_MAX_ERROR_STRING];
+    // get the value of broadcast
+    value = (char *) ADIOI_Malloc((MPI_MAX_INFO_VAL+1));
+    mpi_ret=MPI_Info_get(fd->info,hint,MPI_MAX_INFO_VAL,value,&flag);
+    // If there is an error on the info get the rank and the error message
+    if(mpi_ret!=MPI_SUCCESS) {
+        MPI_Error_string(mpi_ret,err_buffer,&resultlen);
+        MPI_Abort(MPI_COMM_WORLD,MPI_ERR_IO);
+        return -1;
+    } else {
+        if(flag) {
+            hint_value = atoi(value);
+        }
+    }
+    ADIOI_Free(value);
+    return hint_value;
+}
+
+void malloc_check(void *test_me,int rank)
+{
+    if(!test_me) {
+        plfs_debug("Rank %d failed a malloc check\n", rank);
+        MPI_Abort(MPI_COMM_WORLD,MPI_ERR_IO);
+    }
+}
+
+void check_stream(int size,int rank)
+{
+    if(size<0) {
+        plfs_debug("Rank %d had a stream with a negative return size\n", rank);
+        MPI_Abort(MPI_COMM_WORLD,MPI_ERR_IO);
+    }
+}
--- openmpi/ompi/mca/io/romio/romio/adio/ad_plfs/ad_plfs_close.c	1969-12-31 17:00:00.000000000 -0700
+++ openmpi-plfs-patch/ompi/mca/io/romio/romio/adio/ad_plfs/ad_plfs_close.c	2014-10-15 11:15:40.491472000 -0600
@@ -0,0 +1,242 @@
+/* -*- Mode: C; c-basic-offset:4 ; -*- */
+/*
+ *   $Id: ad_plfs_close.c,v 1.9 2004/10/04 15:51:01 robl Exp $
+ *
+ *   Copyright (C) 1997 University of Chicago.
+ *   See COPYRIGHT notice in top-level directory.
+ */
+
+#include "ad_plfs.h"
+
+#include <sys/types.h>
+#include <sys/stat.h>
+#include <unistd.h>
+
+int flatten_then_close(ADIO_File, Plfs_fd *fd,int rank,int amode,int procs,
+                       Plfs_close_opt *close_opt,const char *filename,uid_t);
+void check_error(int err,int rank);
+void reduce_meta(ADIO_File, Plfs_fd *fd,const char *filename,
+                 Plfs_close_opt *close_opt, int rank);
+
+
+void ADIOI_PLFS_Close(ADIO_File fd, int *error_code)
+{
+    int err, rank, amode,procs;
+    plfs_error_t plfs_err;
+    int num_open_handles;
+    static char myname[] = "ADIOI_PLFS_CLOSE";
+    uid_t uid = geteuid();
+    Plfs_close_opt close_opt;
+    close_opt.pinter=PLFS_MPIIO;
+    int flatten=0;
+    plfs_debug("%s: begin\n", myname );
+    MPI_Comm_rank( fd->comm, &rank );
+    MPI_Comm_size( fd->comm, &procs);
+    #if 0 /* TODO: original code - probably an error, but it is never used. */
+      close_opt.num_procs = &procs;
+    #else
+      close_opt.num_procs = procs;
+    #endif
+    close_opt.num_procs = procs;
+    amode = ad_plfs_amode( fd->access_mode );
+    if(fd->fs_ptr==NULL) {
+        // ADIO does a weird thing where it
+        // passes CREAT to just 0 and then
+        // immediately closes.  When we handle
+        // the CREAT, we put a NULL in
+        *error_code = MPI_SUCCESS;
+        return;
+    }
+    double start_time,end_time;
+    start_time=MPI_Wtime();
+    if (plfs_get_filetype(fd->filename) != CONTAINER) {
+        plfs_err = plfs_close(fd->fs_ptr, rank, uid,amode,NULL,&num_open_handles);
+        err = -(plfs_error_to_errno(plfs_err));
+    }else{
+        flatten = ad_plfs_hints (fd , rank, "plfs_flatten_close");
+        if(flatten && fd->access_mode!=ADIO_RDONLY) {
+            // flatten on close takes care of calling plfs_close and setting
+            // up the close_opt
+            close_opt.valid_meta=0;
+            plfs_debug("Rank: %d in flatten then close\n",rank);
+            err = flatten_then_close(fd, fd->fs_ptr, rank, amode, procs, &close_opt,
+                                     fd->filename,uid);
+        } else {
+            // for ADIO, just 0 creates the openhosts and the meta dropping
+            // Grab the last offset and total bytes from all ranks and reduce to max
+            plfs_debug("Rank: %d in regular close\n",rank);
+            if(fd->access_mode!=ADIO_RDONLY) {
+                reduce_meta(fd, fd->fs_ptr, fd->filename, &close_opt, rank);
+            }
+            plfs_err = plfs_close(fd->fs_ptr, rank, uid,amode,&close_opt,&num_open_handles);
+            err = -(plfs_error_to_errno(plfs_err));
+        }
+    } 
+    end_time=MPI_Wtime();
+    plfs_debug("%d: close time: %.2f\n", rank,end_time-start_time);
+    fd->fs_ptr = NULL;
+    if (err < 0 ) {
+        *error_code = MPIO_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE,
+                                           myname, __LINE__, MPI_ERR_IO,
+                                           "**io",
+                                           "**io %s", strerror(-err));
+    } else {
+        *error_code = MPI_SUCCESS;
+    }
+}
+
+
+int flatten_then_close(ADIO_File afd, Plfs_fd *fd,int rank,int amode,int procs,
+                       Plfs_close_opt *close_opt, const char *filename,
+                       uid_t uid)
+{
+    int index_size,err,index_total_size=0,streams_malloc=1,stop_buffer=0;
+    plfs_error_t plfs_err;
+    int num_open_handles;
+    int *index_sizes,*index_disp;
+    char *index_stream,*index_streams;
+    double start_time,end_time;
+    // Get the index stream from the local index
+    container_index_stream(&(fd),&index_stream,&index_size);
+    // Malloc space to receive all of the index sizes
+    // Do all procs need to do this? I think not
+    if(!rank) {
+        index_sizes=(int *)malloc(procs*sizeof(int));
+        if(!index_sizes) {
+            plfs_debug("Malloc failed:index size gather\n");
+            MPI_Abort(MPI_COMM_WORLD,MPI_ERR_IO);
+        }
+    }
+    if(!rank) {
+        start_time=MPI_Wtime();
+    }
+    // Perform the gather of all index sizes to set up our vector call
+    MPI_Gather(&index_size,1,MPI_INT,index_sizes,1,MPI_INT,0,afd->comm);
+    // Figure out how much space we need and then malloc if we are root
+    if(!rank) {
+        end_time=MPI_Wtime();
+        plfs_debug("Gather of index sizes time:%.12f\n"
+                   ,end_time-start_time);
+        int count;
+        // Malloc space for out displacements
+        index_disp=malloc(procs*sizeof(int));
+        if(!index_disp) {
+            plfs_debug("Displacements malloc has failed\n");
+            MPI_Abort(MPI_COMM_WORLD,MPI_ERR_IO);
+        }
+        for(count=0; count<procs; count++) {
+            index_disp[count]=index_total_size;
+            // Calculate the size of the index
+            index_total_size+=index_sizes[count];
+            if(index_sizes[count]==-1) {
+                plfs_debug("Rank %d had an index that wasn't buffered\n",count);
+                stop_buffer=1;
+            }
+        }
+        plfs_debug("Total size of indexes %d\n",index_total_size);
+        if(!stop_buffer) {
+            index_streams=(char *)malloc((index_total_size*sizeof(char)));
+            if(!index_streams) {
+                plfs_debug("Malloc failed:index streams\n");
+                streams_malloc=0;
+            }
+        }
+    }
+    err=MPI_Bcast(&streams_malloc,1,MPI_INT,0,afd->comm);
+    check_error(err,rank);
+    err=MPI_Bcast(&stop_buffer,1,MPI_INT,0,afd->comm);
+    check_error(err,rank);
+    if(!rank) {
+        start_time=MPI_Wtime();
+    }
+    // Gather all of the subindexes only if malloc succeeded
+    // and no one stopped buffering
+    if( streams_malloc && !stop_buffer) {
+        MPI_Gatherv(index_stream,index_size,MPI_CHAR,index_streams,
+                    index_sizes,index_disp,MPI_CHAR,0,afd->comm);
+    }
+    if(!rank) {
+        end_time=MPI_Wtime();
+        plfs_debug("Gatherv of indexes:%.12f\n"
+                   ,end_time-start_time);
+    }
+    // We are root lets combine all of our subindexes
+    if(!rank && streams_malloc && !stop_buffer) {
+        plfs_debug("About to merge indexes for %s\n",filename);
+        start_time=MPI_Wtime();
+        container_merge_indexes(&(fd),index_streams,index_sizes,procs);
+        end_time=MPI_Wtime();
+        plfs_debug("Finished merging indexes time:%.12f\n"
+                   ,end_time-start_time);
+        start_time=MPI_Wtime();
+        plfs_flatten_index(fd,filename);
+        end_time=MPI_Wtime();
+        plfs_debug("Finished flattening time:%.12f\n"
+                   ,end_time-start_time);
+    }
+    if(stop_buffer) {
+        reduce_meta(afd, fd, filename, close_opt, rank);
+    }
+    // Close normally
+    // This should be fine before the previous if statement
+    plfs_err = plfs_close(fd, rank, uid, amode,close_opt, &num_open_handles);
+    if(index_size>0) {
+        free(index_stream);
+    }
+    if(!rank) {
+        // Only root needs to complete these frees
+        free(index_sizes);
+        free(index_disp);
+        if(streams_malloc && !stop_buffer) {
+            free(index_streams);
+        }
+    }
+    // Everyone needs to free their index stream
+    // Root doesn't really need to make this call
+    // Could take out the container_index_stream call for root
+    // This is causing errors does the free to index streams clean this up?
+    return (plfs_err == PLFS_SUCCESS) ? num_open_handles : -(plfs_error_to_errno(plfs_err));
+}
+
+void reduce_meta(ADIO_File afd, Plfs_fd *fd,const char *filename,
+                 Plfs_close_opt *close_opt, int rank)
+{
+    int BLKSIZE=512;
+    struct stat buf;
+    size_t glbl_tot_byt=0;
+    int size_only=1, lazy_stat=1;
+    long long tmp_buf;
+    plfs_query(fd, NULL, NULL, NULL, &lazy_stat);
+    if (lazy_stat == 0) {
+        // every rank calls plfs_sync to flush in-memory index.
+        plfs_sync(fd);
+        plfs_barrier(afd->comm,rank);
+        // rank 0 does slow stat, need not BCAST here
+        if (rank == 0) {
+            size_only = 0;
+            plfs_getattr(fd, filename, &buf, size_only);
+            close_opt->last_offset = (size_t)buf.st_size;
+            glbl_tot_byt = (size_t)buf.st_blocks;
+        }
+    } else {
+        plfs_getattr(fd, filename, &buf, size_only);
+        MPI_Reduce(&(buf.st_size),&tmp_buf,1,MPI_LONG_LONG,MPI_MAX,0,afd->comm);
+        close_opt->last_offset = (off_t)tmp_buf;
+        MPI_Reduce(&(buf.st_blocks),&tmp_buf,1,MPI_LONG_LONG,MPI_SUM,
+                   0,afd->comm);
+        glbl_tot_byt = (size_t)tmp_buf;
+    }
+    close_opt->total_bytes=glbl_tot_byt*BLKSIZE;
+    close_opt->valid_meta=1;
+}
+
+void check_error(int err,int rank)
+{
+    if(err != MPI_SUCCESS) {
+        int resultlen;
+        char err_buffer[MPI_MAX_ERROR_STRING];
+        MPI_Error_string(err,err_buffer,&resultlen);
+        printf("Error:%s | Rank:%d\n",err_buffer,rank);
+        MPI_Abort(MPI_COMM_WORLD,MPI_ERR_IO);
+    }
+}
--- openmpi/ompi/mca/io/romio/romio/adio/ad_plfs/ad_plfs_delete.c	1969-12-31 17:00:00.000000000 -0700
+++ openmpi-plfs-patch/ompi/mca/io/romio/romio/adio/ad_plfs/ad_plfs_delete.c	2014-10-15 11:15:40.492470000 -0600
@@ -0,0 +1,26 @@
+/* -*- Mode: C; c-basic-offset:4 ; -*- */
+/*
+ *   $Id: ad_plfs_delete.c,v 1.1 2010/11/29 19:59:01 adamm Exp $
+ *
+ *   Copyright (C) 1997 University of Chicago.
+ *   See COPYRIGHT notice in top-level directory.
+ */
+
+#include "ad_plfs.h"
+#include "adio.h"
+
+void ADIOI_PLFS_Delete(char *filename, int *error_code)
+{
+    plfs_error_t err;
+    static char myname[] = "ADIOI_PLFS_DELETE";
+    plfs_debug("%s: begin\n", myname );
+    err = plfs_unlink(filename);
+    if (err != PLFS_SUCCESS) {
+        *error_code = MPIO_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE,
+                                           myname, __LINE__, MPI_ERR_IO,
+                                           "**io",
+                                           "**io %s", strplfserr(err));
+    } else {
+        *error_code = MPI_SUCCESS;
+    }
+}
--- openmpi/ompi/mca/io/romio/romio/adio/ad_plfs/ad_plfs_fcntl.c	1969-12-31 17:00:00.000000000 -0700
+++ openmpi-plfs-patch/ompi/mca/io/romio/romio/adio/ad_plfs/ad_plfs_fcntl.c	2014-10-15 11:15:40.493474000 -0600
@@ -0,0 +1,80 @@
+/* -*- Mode: C; c-basic-offset:4 ; -*- */
+/*
+ *
+ *   Copyright (C) 1997 University of Chicago.
+ *   See COPYRIGHT notice in top-level directory.
+ */
+
+#include "ad_plfs.h"
+
+void ADIOI_PLFS_Fcntl(ADIO_File fd, int flag, ADIO_Fcntl_t *fcntl_struct,
+                      int *error_code)
+{
+    static char myname[] = "ADIOI_PLFS_FCNTL";
+    struct stat buf;
+    int rank, size_only, lazy_stat=1;
+    plfs_error_t ret;
+    plfs_debug( "%s: begin\n", myname );
+    switch(flag) {
+    case ADIO_FCNTL_GET_FSIZE:
+        plfs_query(fd->fs_ptr, NULL, NULL, NULL, &lazy_stat);
+        if (lazy_stat == 0) {
+            // every rank calls plfs_sync to flush in-memory index.
+            // this is not a collective operation, take out
+            // all collective calls.
+            plfs_sync(fd->fs_ptr);
+            //plfs_barrier(fd->comm,rank);
+            // rank 0 does slow stat and broadcasts to all.
+            //MPI_Comm_rank(fd->comm, &rank);
+            //if (rank == 0) {
+            size_only = 0;
+            ret = plfs_getattr( fd->fs_ptr, fd->filename, &buf, size_only );
+            //}
+            //MPI_Bcast(&ret, 1, MPI_INT, 0, fd->comm);
+            if (ret == PLFS_SUCCESS) {
+                //MPI_Bcast(&(buf.st_size), 1, MPI_LONG_LONG, 0, fd->comm);
+                fcntl_struct->fsize = buf.st_size;
+                *error_code = MPI_SUCCESS;
+            } else {
+                *error_code = MPIO_Err_create_code(MPI_SUCCESS,
+                                                   MPIR_ERR_RECOVERABLE, myname,
+                                                   __LINE__, MPI_ERR_IO, "**io",
+                                                   "**io %s", strplfserr(ret));
+            }
+        } else {
+            size_only = 1;  // do lazy stat
+            ret = plfs_getattr( fd->fs_ptr, fd->filename, &buf, size_only );
+            if ( ret == PLFS_SUCCESS ) {
+                //This is a non collective version of the commented
+                //code below
+                fcntl_struct->fsize = buf.st_size;
+                //These all depend on a collective call
+                //long long tmp_buf;
+                //MPI_Allreduce(&(buf.st_size), &tmp_buf, 1,
+                //              MPI_LONG_LONG, MPI_MAX, fd->comm);
+                //fcntl_struct->fsize = (size_t)tmp_buf;
+                *error_code = MPI_SUCCESS;
+            } else {
+                *error_code = MPIO_Err_create_code(MPI_SUCCESS,
+                                                   MPIR_ERR_RECOVERABLE, myname,
+                                                   __LINE__, MPI_ERR_IO, "**io",
+                                                   "**io %s", strplfserr(ret));
+            }
+        }
+        //if (fd->fp_sys_posn != -1) {
+        //     pvfs_lseek64(fd->fd_sys, fd->fp_sys_posn, SEEK_SET);
+        //}
+        break;
+    case ADIO_FCNTL_SET_DISKSPACE:
+    case ADIO_FCNTL_SET_ATOMICITY:
+    default:
+        /* --BEGIN ERROR HANDLING-- */
+        *error_code = MPIO_Err_create_code(MPI_SUCCESS,
+                                           MPIR_ERR_RECOVERABLE,
+                                           myname, __LINE__,
+                                           MPI_ERR_ARG,
+                                           "**flag", "**flag %d", flag);
+        return;
+        /* --END ERROR HANDLING-- */
+    }
+}
--- openmpi/ompi/mca/io/romio/romio/adio/ad_plfs/ad_plfs_features.c	1969-12-31 17:00:00.000000000 -0700
+++ openmpi-plfs-patch/ompi/mca/io/romio/romio/adio/ad_plfs/ad_plfs_features.c	2014-10-15 11:15:40.495468000 -0600
@@ -0,0 +1,34 @@
+/*
+ *   Copyright 2011 Cray Inc. All Rights Reserved.
+ */
+/* 
+ *   Copyright (C) 2008 University of Chicago. 
+ *   See COPYRIGHT notice in top-level directory.
+ */
+
+#include "adio.h"
+
+#ifndef ADIO_UNLINK_AFTER_CLOSE
+#define ADIO_UNLINK_AFTER_CLOSE -100
+#endif
+
+#ifndef ROMIO_OPENMPI_14x
+int ADIOI_PLFS_Feature(ADIO_File fd, int flag)
+{
+	switch(flag) {
+		case ADIO_LOCKS:
+		case ADIO_SHARED_FP:
+		case ADIO_ATOMIC_MODE:
+		/* In the case of PLFS, "ADIO_SCALABLE_OPEN==TRUE" translates
+		 * to "don't do deferred open". */
+		case ADIO_SCALABLE_OPEN:
+        case ADIO_UNLINK_AFTER_CLOSE:
+			return 1;
+			break;
+		case ADIO_DATA_SIEVING_WRITES:
+		default:
+			return 0;
+			break;
+	}
+}
+#endif
--- openmpi/ompi/mca/io/romio/romio/adio/ad_plfs/ad_plfs_flush.c	1969-12-31 17:00:00.000000000 -0700
+++ openmpi-plfs-patch/ompi/mca/io/romio/romio/adio/ad_plfs/ad_plfs_flush.c	2014-10-15 11:15:40.496468000 -0600
@@ -0,0 +1,29 @@
+/* -*- Mode: C; c-basic-offset:4 ; -*- */
+/*
+ *   $Id: ad_plfs_flush.c,v 1.1 2010/11/29 19:59:01 adamm Exp $
+ *
+ *   Copyright (C) 1997 University of Chicago.
+ *   See COPYRIGHT notice in top-level directory.
+ */
+
+#include "ad_plfs.h"
+
+void ADIOI_PLFS_Flush(ADIO_File fd, int *error_code)
+{
+    plfs_error_t err;
+    int rank;
+    static char myname[] = "ADIOI_PLFS_FLUSH";
+    plfs_debug( "%s: begin\n", myname );
+    MPI_Comm_rank(fd->comm, &rank);
+    // even though this is a collective routine, everyone must flush here
+    // because everyone has there own data file handle
+    err = plfs_sync(fd->fs_ptr);
+    if (err != PLFS_SUCCESS) {
+        *error_code = MPIO_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE,
+                                           myname, __LINE__, MPI_ERR_IO,
+                                           "**io",
+                                           "**io %s", strplfserr(err));
+    } else {
+        *error_code = MPI_SUCCESS;
+    }
+}
--- openmpi/ompi/mca/io/romio/romio/adio/ad_plfs/ad_plfs.h	1969-12-31 17:00:00.000000000 -0700
+++ openmpi-plfs-patch/ompi/mca/io/romio/romio/adio/ad_plfs/ad_plfs.h	2014-10-15 11:15:40.497469000 -0600
@@ -0,0 +1,84 @@
+/* -*- Mode: C; c-basic-offset:4 ; -*- */
+/*
+ *   $Id: ad_plfs.h,v 1.1 2010/11/29 19:59:01 adamm Exp $
+ *
+ *   Copyright (C) 1997 University of Chicago.
+ *   See COPYRIGHT notice in top-level directory.
+ */
+
+#ifndef AD_PLFS_INCLUDE
+#define AD_PLFS_INCLUDE
+
+#ifndef ROMIOCONF_H_INCLUDED
+#include "romioconf.h"
+#define ROMIOCONF_H_INCLUDED
+#endif
+#ifdef ROMIO_PLFS_NEEDS_INT64_DEFINITION
+typedef long long int int64_t;
+#endif
+
+#include <unistd.h>
+#include <sys/types.h>
+#include <sys/uio.h>
+#include <fcntl.h>
+#include <plfs.h>
+#include <plfs/plfs_internal.h>
+#include "adio.h"
+
+
+void ADIOI_PLFS_Open(ADIO_File fd, int *error_code);
+void ADIOI_PLFS_Close(ADIO_File fd, int *error_code);
+void ADIOI_PLFS_ReadContig(ADIO_File fd, void *buf, int count,
+                           MPI_Datatype datatype, int file_ptr_type,
+                           ADIO_Offset offset, ADIO_Status *status, int
+                           *error_code);
+void ADIOI_PLFS_WriteContig(ADIO_File fd, void *buf, int count,
+                            MPI_Datatype datatype, int file_ptr_type,
+                            ADIO_Offset offset, ADIO_Status *status, int
+                            *error_code);
+void ADIOI_PLFS_Fcntl(ADIO_File fd, int flag, ADIO_Fcntl_t *fcntl_struct, int
+                      *error_code);
+/*
+void ADIOI_PLFS_WriteStrided(ADIO_File fd, void *buf, int count,
+               MPI_Datatype datatype, int file_ptr_type,
+               ADIO_Offset offset, ADIO_Status *status, int
+               *error_code);
+void ADIOI_PLFS_ReadStrided(ADIO_File fd, void *buf, int count,
+               MPI_Datatype datatype, int file_ptr_type,
+               ADIO_Offset offset, ADIO_Status *status, int
+               *error_code);
+void ADIOI_PLFS_SetInfo(ADIO_File fd, MPI_Info users_info, int *error_code);
+*/
+void ADIOI_PLFS_Flush(ADIO_File fd, int *error_code);
+void ADIOI_PLFS_Delete(char *filename, int *error_code);
+void ADIOI_PLFS_Resize(ADIO_File fd, ADIO_Offset size, int *error_code);
+void ADIOI_PLFS_SetInfo(ADIO_File fd, MPI_Info users_info, int *error_code);
+#ifndef ROMIO_OPENMPI_14x
+int  ADIOI_PLFS_Feature(ADIO_File fd, int flag);
+#endif
+
+int plfs_protect_all(const char *file, MPI_Comm comm);
+
+#define plfs_barrier(X,Y) \
+    do { \
+        plfs_debug("Rank %d: Enter barrier from %s:%d", \
+            Y, __FUNCTION__, __LINE__ ); \
+        MPI_Barrier(X); \
+        plfs_debug("Rank %d: Exit barrier from %s:%d", \
+            Y, __FUNCTION__, __LINE__ ); \
+    } while(0);
+    
+
+int ad_plfs_amode( int access_mode );
+void malloc_check(void *test_me,int rank);
+void check_stream(int size,int rank);
+/* Check for hints passed from the command line
+ * Current hints
+ * plfs_enable_broadcast : Turn broadcast of index from root on
+ * plfs_compress_index   : Compress indexes before sending out
+ *                         useless if broadcast off
+ * plfs_flatten_close    : Flatten the index on the close
+ *
+ */
+int ad_plfs_hints(ADIO_File fd, int rank, char *hint);
+#endif
--- openmpi/ompi/mca/io/romio/romio/adio/ad_plfs/ad_plfs_hints.c	1969-12-31 17:00:00.000000000 -0700
+++ openmpi-plfs-patch/ompi/mca/io/romio/romio/adio/ad_plfs/ad_plfs_hints.c	2014-10-15 11:15:40.498475000 -0600
@@ -0,0 +1,93 @@
+/*
+ *   Copyright 2011 Cray Inc. All Rights Reserved.
+ */
+/*   TODO: other copyrights are probably appropriate. */
+
+#include "ad_plfs.h"
+
+#define POORMANS_GDB \
+    printf("%d in %s:%d\n", rank, __FUNCTION__,__LINE__);
+#ifdef ROMIO_CRAY
+#include "../ad_cray/ad_cray.h"
+#endif /* ROMIO_CRAY */
+
+
+void ADIOI_PLFS_SetInfo(ADIO_File fd, MPI_Info users_info, int *error_code)
+{
+    static char myname[] = "ADIOI_PLFS_SETINFO";
+    char *value;
+    int flag, tmp_val = -1, save_val = -1;  
+    int rank, i, gen_error_code;
+
+    MPI_Comm_rank( fd->comm, &rank );
+    *error_code = MPI_SUCCESS;
+    #ifdef ROMIO_CRAY
+       /* Process any hints set with the MPICH_MPIIO_HINTS
+          environment variable. */
+       ADIOI_CRAY_getenv_mpiio_hints(&users_info, fd);
+    #endif /* ROMIO_CRAY */
+
+        // here's the way to check whether we're in container mode
+        // in case we want to error out if people try non-sensical hints
+    if (plfs_get_filetype(fd->filename) != CONTAINER) {
+        // not currently do any checking here
+    }
+
+    // if the hint structure hasn't already been created
+    // however, if users_info==MPI_INFO_NULL, maybe we don't need to do this
+    if ((fd->info) == MPI_INFO_NULL) {
+        MPI_Info_create(&(fd->info));
+    }
+
+    /*
+     * For every plfs hint, go through and check whether every rank
+     * got the same value.  If not, abort.  Then copy the value from
+     * the incoming MPI_Info into the ADIO_File info.
+     */
+    if (users_info != MPI_INFO_NULL) {
+        static const char *phints[] = { 
+            "plfs_disable_broadcast",   /* don't have 0 broadcast to all */ 
+            "plfs_flatten_close",       /* create flattened index on close */
+            "plfs_disable_paropen",     /* don't do par_index_read */
+            "plfs_uniform_restart",     /* only read one index file each */ 
+            NULL    /* last must be NULL */
+        };
+
+        value = (char *) ADIOI_Malloc((MPI_MAX_INFO_VAL+1)*sizeof(char));
+        for(i = 0; phints[i] != NULL; i++) {
+            MPI_Info_get(users_info, (char *)phints[i], MPI_MAX_INFO_VAL, value, &flag);
+            if (flag) {
+                save_val = tmp_val = atoi(value);
+                MPI_Bcast(&tmp_val, 1, MPI_INT, 0, fd->comm);
+                if (tmp_val != save_val) {  /* same for all? */
+                    FPRINTF(stderr, "%s: " "the value for key \"%s\" must be "
+                            "the same on all processes\n", myname, phints[i]);
+                    MPI_Abort(MPI_COMM_WORLD, 1);
+                }
+                MPI_Info_set(fd->info, (char *)phints[i], value);
+                //fprintf(stderr, "rank %d: set %s -> %s\n",rank,phints[i],value);
+            }
+        }
+        ADIOI_Free(value);
+    }
+    
+    #ifdef ROMIO_CRAY /* --BEGIN CRAY ADDITION-- */
+        /* Calling the CRAY SetInfo() will add the Cray supported features:
+         * - set the number of aggregators to the number of compute nodes
+         * - MPICH_MPIIO_HINTS environment variable
+         * - MPICH_MPIIO_HINTS_DISPLAY env var to display of hints values
+         * - etc
+         */
+        ADIOI_CRAY_SetInfo(fd, users_info, &gen_error_code); 
+    #else
+        ADIOI_GEN_SetInfo(fd, users_info, &gen_error_code); 
+    #endif /* --END CRAY ADDITION-- */
+
+    /* If this function is successful, use the error code
+     * returned from ADIOI_GEN_SetInfo
+     * otherwise use the error_code generated by this function
+     */
+    if(*error_code == MPI_SUCCESS) {
+        *error_code = gen_error_code;
+    }
+}
--- openmpi/ompi/mca/io/romio/romio/adio/ad_plfs/ad_plfs_open.c	1969-12-31 17:00:00.000000000 -0700
+++ openmpi-plfs-patch/ompi/mca/io/romio/romio/adio/ad_plfs/ad_plfs_open.c	2014-10-15 11:15:40.500470000 -0600
@@ -0,0 +1,968 @@
+/* -*- Mode: C; c-basic-offset:4 ; -*- */
+/*
+ *   $Id: ad_plfs_open.c,v 1.1 2010/11/29 19:59:01 adamm Exp $
+ *
+ *   Copyright (C) 1997 University of Chicago.
+ *   See COPYRIGHT notice in top-level directory.
+ */
+#include "ad_plfs.h"
+#include "zlib.h"
+#include <dirent.h>
+#include <string.h>
+#include <limits.h>
+#include <assert.h>
+
+
+#define VERBOSE_DEBUG 0
+#define BIT_ARRAY_LENGTH MAX_HOSTDIRS
+
+
+typedef char Bitmap;
+Bitmap bitmap[BIT_ARRAY_LENGTH/CHAR_BIT];
+
+// check whether bit is set in our bitmap
+int
+adplfs_bitIsSet( long n, char *bitmap )
+{
+    long whichByte = n / CHAR_BIT;
+    int  whichBit  = n % CHAR_BIT;
+    return ((bitmap[whichByte] << whichBit) & 0x80) != 0;
+}
+
+// set a bit in a bitmap
+void
+adplfs_setBit( long n, char *bitmap )
+{
+    long whichByte = n / CHAR_BIT;
+    int  whichBit  = n % CHAR_BIT;
+    char temp = bitmap[whichByte];
+    bitmap[whichByte] = (char)(temp | (0x80 >> whichBit));
+}
+
+// clear a bit in a bitmap
+void
+adplfs_clearBit( long n, char *bitmap )
+{
+    long whichByte = n / CHAR_BIT;
+    int  whichBit  = n % CHAR_BIT;
+    char temp = bitmap[whichByte];
+    bitmap[whichByte] = (char)(temp & ~(0x80 >> whichBit));
+}
+
+// A bitmap to hold the number of and id of
+// hostdirs inside of the container
+/*
+typedef struct {
+    unsigned int bit:1;
+}Bit;
+Bit bitmap[BIT_ARRAY_LENGTH]={0};
+*/
+
+// a bunch of helper macros we added when we had a really hard time debugging
+// this file.  We were confused by ADIO calling rank 0 initially on the create
+// and then again on the open (and a bunch of other stuff)
+#if VERBOSE_DEBUG == 1
+#define BITMAP_PRINT adplfs_host_list_print(__LINE__,bitmap);
+
+#define POORMANS_GDB \
+        fprintf(stderr,"%d in %s:%d\n", rank, __FUNCTION__,__LINE__);
+
+#define TEST_BCAST(X) \
+    {\
+        int test = -X;\
+        if(rank==0) { \
+            test = X; \
+        }             \
+        MPIBCAST( &test, 1, MPI_INT, 0, fd->comm );\
+        fprintf(stderr,"rank %d got test %d\n",rank,test);\
+        if(test!=X){ \
+            MPI_Abort(MPI_COMM_WORLD,MPI_ERR_IO);\
+        }\
+    }
+#else
+#define BITMAP_PRINT {}
+#define POORMANS_GDB {}
+#define TEST_BCAST(X) {}
+#endif
+
+#define MPIBCAST(A,B,C,D,E) \
+    POORMANS_GDB \
+    { \
+        ret = MPI_Bcast(A,B,C,D,E); \
+        if(ret!=MPI_SUCCESS) { \
+            int resultlen; \
+            char err_buffer[MPI_MAX_ERROR_STRING]; \
+            MPI_Error_string(ret,err_buffer,&resultlen); \
+            printf("Error:%s | Rank:%d\n",err_buffer,rank); \
+            MPI_Abort(MPI_COMM_WORLD,MPI_ERR_IO); \
+        } \
+    } \
+    POORMANS_GDB
+
+#define MPIALLGATHER(A,B,C,D,E,F,G)\
+{\
+    ret = MPI_Allgather(A,B,C,D,E,F,G);\
+    if(ret!= MPI_SUCCESS){\
+        int resultlen; \
+        char err_buffer[MPI_MAX_ERROR_STRING]; \
+        MPI_Error_string(ret,err_buffer,&resultlen); \
+        printf("Error:%s | Rank:%d\n",err_buffer,rank); \
+        MPI_Abort(MPI_COMM_WORLD,MPI_ERR_IO); \
+    } \
+} \
+ 
+#define MPIALLGATHERV(A,B,C,D,E,F,G,H)\
+{\
+    ret = MPI_Allgatherv(A,B,C,D,E,F,G,H);\
+    if(ret!= MPI_SUCCESS){\
+        int resultlen; \
+        char err_buffer[MPI_MAX_ERROR_STRING]; \
+        MPI_Error_string(ret,err_buffer,&resultlen); \
+        printf("Error:%s | Rank:%d\n",err_buffer,rank); \
+        MPI_Abort(MPI_COMM_WORLD,MPI_ERR_IO); \
+    } \
+}
+
+#define MPIGATHER(A,B,C,D,E,F,G,H)\
+{\
+    ret = MPI_Gather(A,B,C,D,E,F,G,H);\
+    if(ret!= MPI_SUCCESS){\
+        int resultlen; \
+        char err_buffer[MPI_MAX_ERROR_STRING]; \
+        MPI_Error_string(ret,err_buffer,&resultlen); \
+        printf("Error:%s | Rank:%d\n",err_buffer,rank); \
+        MPI_Abort(MPI_COMM_WORLD,MPI_ERR_IO); \
+    } \
+}
+
+#define MPIGATHERV(A,B,C,D,E,F,G,H,I)\
+{\
+    ret = MPI_Gatherv(A,B,C,D,E,F,G,H,I);\
+    if(ret!= MPI_SUCCESS){\
+        int resultlen; \
+        char err_buffer[MPI_MAX_ERROR_STRING]; \
+        MPI_Error_string(ret,err_buffer,&resultlen); \
+        printf("Error:%s | Rank:%d\n",err_buffer,rank); \
+        MPI_Abort(MPI_COMM_WORLD,MPI_ERR_IO); \
+    } \
+}
+#define MPITYPESIZE(A,B) \
+{ \
+        ret = MPI_Type_size(A,B); \
+        if(ret!=MPI_SUCCESS) { \
+            int resultlen; \
+            char err_buffer[MPI_MAX_ERROR_STRING]; \
+            MPI_Error_string(ret,err_buffer,&resultlen); \
+            printf("Error:%s | Rank:%d\n",err_buffer,rank); \
+            MPI_Abort(MPI_COMM_WORLD,MPI_ERR_IO); \
+        } \
+}
+
+int adplfs_open_helper(ADIO_File fd,Plfs_fd **pfd,int *error_code,int perm,
+                int amode,int rank);
+int adplfs_broadcast_index(Plfs_fd **pfd, ADIO_File fd,
+                    int *error_code,int perm,int amode,int rank,
+                    int compress_flag);
+int adplfs_getPerm(ADIO_File);
+
+int adplfs_getPerm(ADIO_File fd)
+{
+    int perm = fd->perm;
+    if (fd->perm == ADIO_PERM_NULL) {
+        int old_mask = umask(022);
+        umask(old_mask);
+        perm = old_mask ^ 0666;
+    }
+    return perm;
+}
+// Par index read stuff
+int adplfs_par_index_read(ADIO_File fd,Plfs_fd **pfd,int *error_code,int perm,
+                   int amode,int rank, void **global_index);
+// Printer for the bitmap struct
+void adplfs_host_list_print(int line, Bitmap *bitmap);
+// Function to calculate the extra ranks
+int adplfs_extra_rank_calc(int np,int num_host_dir);
+// Number of ranks per comm
+int adplfs_ranks_per_comm_calc(int np,int num_host_dir);
+// Based on my rank how many ranks are in my hostdir comm
+int adplfs_rank_to_size(int rank,int ranks_per_comm,int extra_rank,
+                 int np,int group_index);
+// Index used for a color that determines my hostdir comm
+int adplfs_rank_to_group_index(int rank,int ranks_per_comm,int extra_rank);
+// Converts bitmap position to a dirname
+char *adplfs_bitmap_to_dirname(Bitmap *bitmap,int group_index,
+                        char *target,int mult,int np);
+// Called when num procs >= num_host_dirs
+void adplfs_split_and_merge(ADIO_File fd,int rank,int extra_rank,
+                            int ranks_per_comm,int np,char *filename,
+                            void *pmount, void *pback, void **global_index);
+// Called when num hostdirs > num procs
+void adplfs_read_and_merge(ADIO_File fd,int rank,
+                           int np,int hostdir_per_rank,char *filename,
+                           void *pmount, void *pback, void **global_index);
+// Added to handle the case where one rank must read more than one hostdir
+char *adplfs_count_to_hostdir(Bitmap *bitmap,int stop_point,int *count,
+                       int *hostdir_found,char *filename,char *target,
+                       int first);
+// Broadcast the bitmap to interested parties
+void adplfs_bcast_bitmap(MPI_Comm comm,int rank);
+
+void ADIOI_PLFS_Open(ADIO_File fd, int *error_code)
+{
+    Plfs_fd *pfd =NULL;
+    // I think perm is the mode and amode is the flags
+    int err,perm, amode, old_mask,rank,ret;
+    plfs_error_t plfs_err = PLFS_SUCCESS;
+    int num_open_handles;
+    MPI_Comm_rank( fd->comm, &rank );
+    static char myname[] = "ADIOI_PLFS_OPEN";
+    perm = adplfs_getPerm(fd);
+    amode = ad_plfs_amode(fd->access_mode);
+    // ADIO makes 2 calls into here:
+    // first, just 0 with CREATE
+    // then everyone without
+
+    //check for existence of file
+    struct stat buffer;
+
+    if (fd->access_mode &  MPI_MODE_EXCL){
+        //if MPI_MODE_EXCL is set, check if the file already exists
+        //will stat either an empty file, or a file that exists and die
+        if (plfs_access(fd->filename, F_OK) == PLFS_SUCCESS){
+            //throw an error if the file exists
+            *error_code = MPIO_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE,
+                myname, __LINE__, MPI_ERR_IO,
+                "**io",
+                "**io %s", strplfserr(plfs_err));
+            return;
+        }
+    }
+    if (fd->access_mode & ADIO_CREATE) {
+        //rather then call creat directly, call open w/ O_CREAT to avoid truncate
+        //of existing files
+        plfs_err = plfs_open(&pfd, fd->filename, amode|O_CREAT, rank, perm, NULL);
+        if ( plfs_err != PLFS_SUCCESS ) {
+            *error_code = MPIO_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE,
+                                              myname, __LINE__, MPI_ERR_IO,
+                                              "**io",
+                                              "**io %s", strplfserr(plfs_err));
+            errno = plfs_error_to_errno(plfs_err);
+        } else {
+            //passing pfd as the fs_ptr will result in a hang n-1 with many
+            //procs so close instead if open
+            *error_code = MPI_SUCCESS;
+            plfs_err = plfs_close(pfd, rank, geteuid(), amode, NULL, &num_open_handles);
+            if ( plfs_err != PLFS_SUCCESS ) {
+                *error_code = MPIO_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE,
+                                                  myname, __LINE__, MPI_ERR_IO,
+                                                  "**io",
+                                                  "**io %s", strplfserr(plfs_err));
+                errno = plfs_error_to_errno(plfs_err);
+            }
+        }
+
+        fd->fs_ptr = NULL;
+        return;
+    }
+
+    // if we make it here, we're doing RDONLY, WRONLY, or RDWR
+    // at this point, we want to do different for container/flat_file mode
+    if (plfs_get_filetype(fd->filename) != CONTAINER) {
+        plfs_err = plfs_open(&pfd,fd->filename,amode,rank,perm,NULL);
+        if ( plfs_err != PLFS_SUCCESS ) {
+            *error_code = MPIO_Err_create_code(MPI_SUCCESS,
+                                               MPIR_ERR_RECOVERABLE,
+                                               myname, __LINE__, MPI_ERR_IO,
+                                               "**io",
+                                               "**io %s", strplfserr(plfs_err));
+            plfs_debug( "%s: failure %s\n", myname, strplfserr(plfs_err) );
+            return;
+        } else {
+            plfs_debug( "%s: Success on open(%d)!\n", myname, rank );
+            fd->fs_ptr = pfd;
+            fd->fd_direct = -1;
+            *error_code = MPI_SUCCESS;
+            return;
+        }
+    }
+    // if we get here, we're in container mode; continue with the optimizations
+    ret = adplfs_open_helper(fd,&pfd,error_code,perm,amode,rank);
+    MPI_Allreduce(&ret, &err, 1, MPI_INT, MPI_MIN, fd->comm);
+    if ( err != 0 ) {
+        *error_code = MPIO_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE,
+                                           myname, __LINE__, MPI_ERR_IO,
+                                           "**io",
+                                           "**io %s", strerror(-err));
+        errno = -err;
+        plfs_debug( "%s: failure %s\n", myname, strerror(-err) );
+        return;
+    } else {
+        plfs_debug( "%s: Success on open (%d)!\n", myname, rank );
+        *error_code = MPI_SUCCESS;
+    }
+    return;
+}
+
+// a helper that determines whether 0 distributes the index to everyone else
+// or whether everyone just calls plfs_open directly
+int adplfs_open_helper(ADIO_File fd,Plfs_fd **pfd,int *error_code,int perm,
+                int amode,int rank)
+{
+    // XXXJB: Looks like compress_flag is not used at all anymore.  Clean?
+    int err = 0, disabl_broadcast=0, compress_flag=0,close_flatten=0;
+    plfs_error_t plfs_err = PLFS_SUCCESS;
+    int parallel_index_read=1;
+    int uniform_restart=0;
+    static char myname[] = "ADIOI_PLFS_OPENHELPER";
+    Plfs_open_opt open_opt;
+    memset(&open_opt, 0, sizeof(Plfs_open_opt));
+    MPI_Comm hostdir_comm;
+    int hostdir_rank, write_mode;
+    open_opt.reopen = 0;
+    open_opt.mdhim_comm = &fd->comm;
+    //mdhim-mod-put at
+    open_opt.mdhim_init =0;
+    //mdhim-mod-put at
+    plfs_debug("XXXACXXX - mpi_adio/ad_plfs/ad_plfs_open::%s: MDHIM Comm set.\n", myname);
+    // get a hostdir comm to use to serialize write a bit
+    write_mode = (fd->access_mode==ADIO_RDONLY?0:1);
+    plfs_debug("XXXACXXX - mpi_adio/ad_plfs/ad_plfs_open::%s: write_mode = %d\n", myname, write_mode);
+    if (write_mode) {
+        char *hostname;
+        plfs_gethostname(&hostname);
+        size_t color = container_gethostdir_id(hostname);
+        err = MPI_Comm_split(fd->comm,color,rank,&hostdir_comm);
+        if(err!=MPI_SUCCESS) {
+            return err;
+        }
+        MPI_Comm_rank(hostdir_comm,&hostdir_rank);
+    }
+    // get specified behavior from hints
+    if (fd->access_mode==ADIO_RDONLY) {
+        plfs_debug("XXXACXXX - mpi_adio/ad_plfs/ad_plfs_open::%s: fd->access_mode==ADIO_RDONLY\n", myname);
+        disabl_broadcast = ad_plfs_hints(fd,rank,"plfs_disable_broadcast");
+        parallel_index_read =!ad_plfs_hints(fd,rank,"plfs_disable_paropen");
+        uniform_restart=ad_plfs_hints(fd,rank,"plfs_uniform_restart");
+        plfs_debug(
+          "Disable_bcast:%d,compress_flag:%d,parindex:%d,uniform_restart:%d\n",
+          disabl_broadcast,compress_flag,parallel_index_read,uniform_restart);
+        // I took out the extra broadcasts at this point. ad_plfs_hints
+        // has code to make sure that all ranks have the same value
+        // for the hint
+    } else {
+        plfs_debug("XXXACXXX - mpi_adio/ad_plfs/ad_plfs_open::%s: fs->access_mode!=ADIO_RDONLY\n", myname);
+        disabl_broadcast = 1; // don't create an index unless we're in read mode
+        compress_flag=0;
+    }
+    if (fd->access_mode == ADIO_RDONLY && uniform_restart){
+        plfs_debug("XXXACXXX - mpi_adio/ad_plfs/ad_plfs_open::%s: fd->access_mode==ADIO_RDONLY && uniform_restart\n", myname);
+        open_opt.uniform_restart_enable = 1;
+        open_opt.uniform_restart_rank = rank;
+        plfs_err = plfs_open(pfd,fd->filename,amode,rank,perm,&open_opt);
+    }else if( fd->access_mode==ADIO_RDONLY && parallel_index_read) {
+        plfs_debug("XXXACXXX - mpi_adio/ad_plfs/ad_plfs_open::%s: fd->access_mode==ADIO_RDONLY && parallel_index_read\n", myname);
+        // mdhim-mod at
+        //void *global_index;
+        //mdhim-mod at
+        // Function to start the parallel index read
+        plfs_debug("XXXACXXX - mpi_adio/ad_plfs/ad_plfs_open::%s: call to adplfs_par_index_read\n", myname);
+        //err = adplfs_par_index_read(fd,pfd,error_code,perm,amode,rank,
+        //                     &global_index);
+        err = 0;
+        // mdhim-mod at
+        if (err == 0) {
+            open_opt.pinter = PLFS_MPIIO;
+        // mdhim-mod at
+            //   open_opt.index_stream=global_index;
+        // mdhim-mod at
+            plfs_debug("XXXACXXX - mpi_adio/ad_plfs/ad_plfs_open::%s: call to plfs_open\n", myname);
+            plfs_err = plfs_open(pfd,fd->filename,amode,rank,perm,&open_opt);
+        // mdhim-mod at
+            //free(global_index);
+        // mdhim-mod at
+        }
+    } else if(fd->access_mode==ADIO_RDONLY && !disabl_broadcast) {
+        plfs_debug("XXXACXXX - mpi_adio/ad_plfs/ad_plfs_open::%s: fd->access_mode==ADIO_RDONLY && !disabl_broadcast\n", myname);
+        // If we are RDONLY and broadcast isn't disabled let's broadcast it
+        err = adplfs_broadcast_index(pfd,fd,error_code,perm,amode,rank,compress_flag);
+    } else {
+        // here we are either writing or reading without optimizations
+        open_opt.pinter = PLFS_MPIIO;
+        open_opt.index_stream=NULL;
+
+        plfs_debug("XXXACXXX - mpi_adio/ad_plfs/ad_plfs_open::%s: call to ad_plfs_hints\n", myname);
+        close_flatten = ad_plfs_hints(fd,rank,"plfs_flatten_close");
+        // Let's only buffer when the flatten on close hint is passed
+        // and we are in WRONLY mode
+        open_opt.buffer_index=close_flatten;
+        plfs_debug("Opening without a broadcast\n");
+        // everyone opens themselves (write mode or independent read mode)
+        // hostdir_rank zeros do the open first on the write
+        if (write_mode && hostdir_rank) {
+            plfs_barrier(hostdir_comm,rank);
+        }
+        plfs_debug("XXXATXXX - mpi_adio/ad_plfs/ad_plfs_open::%s: call to plfs_open\n",myname);
+        plfs_err = plfs_open( pfd, fd->filename, amode, rank, perm ,&open_opt);
+        if (write_mode && !hostdir_rank) {
+            plfs_debug("XXXACXXX - mpi_adio/ad_plfs/ad_plfs_open::%s: call to plfs_barrier\n",myname);
+            plfs_barrier(hostdir_comm,rank);
+        }
+        // XXX AC mdhim-mod
+        if (write_mode) {
+            plfs_barrier(hostdir_comm,rank);
+        }
+        // XXX AC mdhim-mod
+        //mdhim-mod-put at
+        open_opt.mdhim_init = 1;
+        plfs_debug("XXXATXXX - Setting mdhim init flag to 1\n",myname);
+        plfs_err = plfs_open( pfd, fd->filename, amode, rank, perm ,&open_opt);
+        //mdhim-mod-put at
+    }
+    // clean up the communicator we used
+    if (write_mode) {
+        plfs_debug("XXXACXXX - mpi_adio/ad_plfs/ad_plfs_open::%s: call call to MPI_Comm_free\n",myname);
+        MPI_Comm_free(&hostdir_comm);
+    }
+    if (err == 0) {
+        err = -(plfs_error_to_errno(plfs_err));
+    }
+    if ( err < 0 ) {
+        *error_code = MPIO_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE,
+                                           myname, __LINE__, MPI_ERR_IO,
+                                           "**io",
+                                           "**io %s", strerror(-err));
+        plfs_debug( "%s: failure %s\n", myname, strerror(-err) );
+        errno = -err;
+        return err;
+    } else {
+        plfs_debug( "%s: Success on open(%d)!\n", myname, rank );
+        fd->fs_ptr = *pfd;
+        fd->fd_direct = -1;
+        *error_code = MPI_SUCCESS;
+        return 0;
+    }
+}
+
+// 0 gets the index by calling plfs_open() first and then extracting the index
+// it then broadcasts that to the rest who then pass it to their own plfs_open()
+int adplfs_broadcast_index(Plfs_fd **pfd, ADIO_File fd,
+                    int *error_code,int perm,int amode,int rank,
+                    int compress_flag)
+{
+    plfs_error_t err = PLFS_SUCCESS;
+    int ret;
+    char *index_stream;
+    char *compr_index;
+    // [0] is index stream size [1] is compressed size
+    unsigned long index_size[2]= {0};
+    Plfs_open_opt open_opt;
+    memset(&open_opt, 0, sizeof(Plfs_open_opt));
+    open_opt.pinter = PLFS_MPIIO;
+    open_opt.index_stream=NULL;
+    open_opt.buffer_index=0;
+    open_opt.reopen = 0;
+    if(rank==0) {
+        err = plfs_open(pfd, fd->filename, amode, rank, perm , &open_opt);
+    }
+    MPIBCAST(&err,1,MPI_INT,0,fd->comm);   // was 0's open successful?
+    if(err !=PLFS_SUCCESS ) {
+        return err;
+    }
+    // rank 0 turns the index into a stream, broadcasts its size, then it
+    if(rank==0) {
+        plfs_debug("In broadcast index with compress_flag:%d\n",compress_flag);
+        if(container_index_stream(pfd,&index_stream,&(index_size[0])) != PLFS_SUCCESS ) {
+            MPI_Abort(MPI_COMM_WORLD,MPI_ERR_IO);
+        }
+        index_size[1] = index_size[0];
+        if(compress_flag) {
+            plfs_debug("About to malloc the compressed index space\n");
+            compr_index=malloc(index_size[0]);
+            // Check the malloc
+            if(!compr_index) {
+                plfs_debug("Rank %d aborting because of failed malloc\n",rank);
+                MPI_Abort(MPI_COMM_WORLD,MPI_ERR_IO);
+            }
+            plfs_debug("About to compress the index\n");
+        }
+    }
+    // Original index stream size
+    if (!rank) {
+        plfs_debug("Broadcasting the sizes of the index:%d "
+                   "and compressed index%d\n" ,index_size[0],index_size[1]);
+    }
+    MPIBCAST(index_size, 2, MPI_LONG, 0, fd->comm);
+    if(rank!=0) {
+        index_stream = malloc(index_size[0]);
+        if(compress_flag) {
+            compr_index = malloc(index_size[1]);
+            if(!compr_index ) {
+                plfs_debug("Rank %d aborting because of failed malloc\n",rank);
+                MPI_Abort(MPI_COMM_WORLD,MPI_ERR_IO);
+            }
+        }
+        //We need to check that the malloc succeeded or the broadcast is in vain
+        if(!index_stream ) {
+            plfs_debug("Rank %d aborting because of a failed malloc\n",rank);
+            MPI_Abort(MPI_COMM_WORLD,MPI_ERR_IO);
+        }
+    }
+    if (compress_flag) {
+        if(!rank) {
+            plfs_debug("Broadcasting compressed index\n");
+        }
+        MPIBCAST(compr_index,index_size[1],MPI_CHAR,0,fd->comm);
+    } else {
+        if(!rank) {
+            plfs_debug("Broadcasting full index\n");
+        }
+        MPIBCAST(index_stream,index_size[0],MPI_CHAR,0,fd->comm);
+    }
+    // Broadcast compressed index
+    if(rank!=0) {
+        unsigned long uncompr_len=index_size[0];
+        // Uncompress the index
+        if(compress_flag) {
+            plfs_debug("Rank: %d has compr_len %d and expected expanded of %d\n"
+                       ,rank,index_size[1],uncompr_len);
+            // Error if the uncompressed length doesn't match original length
+            if(uncompr_len!=index_size[0]) {
+                plfs_debug("Uncompressed len != original index size\n");
+                MPI_Abort(MPI_COMM_WORLD,MPI_ERR_IO);
+            }
+        }
+        open_opt.index_stream=index_stream;
+        err = plfs_open(pfd,fd->filename,amode,rank,perm,&open_opt);
+    }
+    if(compress_flag) {
+        free(compr_index);
+    }
+    free(index_stream);
+    return 0;
+}
+
+// returns 0 or -errno
+int adplfs_par_index_read(ADIO_File fd,Plfs_fd **pfd,int *error_code,int perm,
+                   int amode,int rank, void **global_index)
+{
+    plfs_debug("XXXACXXX - mpi_adio/ad_plfs/ad_plfs_open::%s\n", __FUNCTION__);
+    // Each rank and the number of processes playing
+    int np,extra_rank,ret,mpi_size;
+    void *pmount, *pback;
+    MPI_Comm_size(fd->comm, &np);
+    // Rank 0 reads the top level directory and sets the
+    // next two variables
+    int num_host_dir=0;
+    // Every other rank can figures this out using num_host_dir
+    int ranks_per_comm=0;
+    // Only used is ranks per comm equals zero
+    int hostdir_per_rank=0;
+    char *filename;  /* bpath to container */
+    plfs_debug("XXXACXXX - mpi_adio/ad_plfs/ad_plfs_open::%s: call to plfs_expand_path\n", __FUNCTION__);
+    plfs_expand_path(fd->filename,&filename,&pmount,&pback);
+    // Rank 0 only code
+    MPITYPESIZE(MPI_CHAR, &mpi_size);
+    int bitmap_bcast_sz = (BIT_ARRAY_LENGTH/CHAR_BIT)/mpi_size;
+    memset(bitmap,0,bitmap_bcast_sz);
+    if(!rank) {
+        // Find out how many hostdirs we currently have
+        // and save info in a bitmap
+        // XXX: why is bitmap global?
+        (void)container_num_host_dirs(&num_host_dir, filename, pback, bitmap);
+        plfs_debug("Num of hostdirs calculated is |%d|\n",num_host_dir);
+    }
+    // Bcast usage brought down by the MPI_Comm_split
+    // Was using MPI_Group_incl which needed an array of
+    // all members of the group. Don't forget to plan and
+    // look at available methods to get the job done
+    MPIBCAST(&num_host_dir,1,MPI_INT,0,fd->comm);
+    if(num_host_dir < 0) {
+        return num_host_dir;
+    }
+    BITMAP_PRINT;
+    // Get some information used to determine the group index
+    ranks_per_comm=adplfs_ranks_per_comm_calc(np,num_host_dir);
+    plfs_debug("%d ranks per comm\n", ranks_per_comm);
+    // Split based on the number of ranks per comm. If zero we
+    // take another path and the extra_rank and hostdir_per_rank
+    // calculation are different
+    if(ranks_per_comm) {
+        extra_rank=adplfs_extra_rank_calc(np,num_host_dir);
+    }
+    if(!ranks_per_comm) {
+        extra_rank=num_host_dir-np;
+        hostdir_per_rank=num_host_dir/np;
+        int left_over=num_host_dir%np;
+        if(rank<left_over) {
+            hostdir_per_rank++;
+        }
+    }
+    plfs_debug("ranks_per_comm=%d,hostdir_per_rank=%d\n",ranks_per_comm,
+               hostdir_per_rank);
+    // Here we split on the ranks per comm. Should not be necessary
+    // to check return values. Functions will abort if an error is encountered
+    if(ranks_per_comm) {
+        plfs_debug("XXXACXXX - mpi_adio/ad_plfs/ad_plfs_open::%s: call to adplfs_split_and_merge\n", __FUNCTION__);
+        adplfs_split_and_merge(fd,rank,extra_rank,
+                               ranks_per_comm,np,filename, pmount, pback,
+                               global_index);
+    }
+    if(!ranks_per_comm) {
+        plfs_debug("XXXACXXX - mpi_adio/ad_plfs/ad_plfs_open::%s: call to adplfs_read_and_merge\n", __FUNCTION__);
+        adplfs_read_and_merge(fd,rank,np,hostdir_per_rank, filename,
+                              pmount, pback, global_index);
+    }
+    if(filename!=NULL) {
+        free(filename);
+    }
+    return 0;
+}
+
+// This is the case where a rank has to read more than one hostdir
+void adplfs_read_and_merge(ADIO_File fd,int rank,
+                           int np,int hostdir_per_rank,char *filename,
+                           void *pmount, void *pback, void **global_index)
+{
+    plfs_debug("XXXACXXX - mpi_adio/ad_plfs/ad_plfs_open::%s:\n", __FUNCTION__);
+    char *targets;
+    int index_sz,ret,count,*index_sizes,*index_disp,index_total_size=0;
+    int global_index_sz;
+    void *index_stream,*index_streams;
+    // Get the bitmap
+    adplfs_bcast_bitmap(fd->comm,rank);
+
+    /*
+     * expands bpath to canonical container to current hostdir set.
+     * there will be multiple hostdirs in "targets" string, sep'd by "|"...
+     * some of them may be metalinks.  the rank value is used to select
+     * which hostdirs are returned in targets.
+     */
+    targets=adplfs_bitmap_to_dirname(bitmap,rank,filename,
+                              hostdir_per_rank,np);
+
+    /*
+     * each rank reads the set of hostdirs assigned to it here.
+     * this results in a merged index stream (index_stream of index_sz
+     * bytes).
+     */
+    plfs_debug("XXXACXXX - mpi_adio/ad_plfs/ad_plfs_open::%s: call to container_hostdir_rddir\n", __FUNCTION__);
+    container_hostdir_rddir(&index_stream,targets,
+                                rank,filename,pmount,pback,&index_sz);
+    // Make sure it was malloced
+    check_stream(index_sz,rank);
+    // Targets no longer needed
+    free(targets);
+
+    // Used to hold the sizes of indexes from all procs
+    // needed for ALLGATHERV
+    index_sizes=malloc(sizeof(int)*np);
+    malloc_check((void *)index_sizes,rank);
+    // Gets the index sizes from all ranks
+    MPIALLGATHER(&index_sz,1,MPI_INT,index_sizes,
+                 1,MPI_INT,fd->comm);
+    // Set up displacements
+    index_disp=malloc(sizeof(int)*np);
+    malloc_check((void *)index_disp,rank);
+    for(count=0; count<np; count++) {
+        index_disp[count]=index_total_size;
+        index_total_size+=index_sizes[count];
+    }
+    // Holds all of the index streams from all procs involved in the process
+    index_streams=malloc(sizeof(char)*index_total_size);
+    malloc_check(index_streams,rank);
+    // ALLGATHERV grabs all of the indexes from all the procs
+    MPIALLGATHERV(index_stream,index_sz,MPI_CHAR,index_streams,index_sizes,
+                  index_disp,MPI_CHAR,fd->comm);
+
+    /*
+     * now each rank has collected the index info from the other ranks
+     * in memory (index_streams).   all that is left to do is construct
+     * a global index by merging all the ranks data into a single index
+     * using container_parindexread_merge().
+     */
+    free(index_stream); // index_stream no longer needed
+    plfs_debug("XXXACXXX - mpi_adio/ad_plfs/ad_plfs_open::%s: call to container_parindexread_merge\n", __FUNCTION__);
+    global_index_sz=container_parindexread_merge(filename,index_streams,
+                                            index_sizes,np,global_index);
+    check_stream(global_index_sz,rank);
+    plfs_debug("Rank |%d| global size |%d|\n",rank,global_index_sz);
+    free(index_streams);
+}
+
+void adplfs_bcast_bitmap(MPI_Comm comm, int rank)
+{
+    int ret;
+    int mpi_size;
+
+    MPITYPESIZE(MPI_CHAR, &mpi_size)
+    int bitmap_bcast_sz = (BIT_ARRAY_LENGTH/CHAR_BIT)/mpi_size;
+    BITMAP_PRINT;
+    MPIBCAST(bitmap,bitmap_bcast_sz,MPI_UNSIGNED_CHAR,0,comm);
+    BITMAP_PRINT;
+      
+}
+
+// If ranks > hostdirs we can split up our comm
+void adplfs_split_and_merge(ADIO_File fd,int rank,int extra_rank,
+                            int ranks_per_comm,int np,char *filename,
+                            void *pmount, void *pback, void **global_index)
+{
+    int new_rank,color,group_index,hc_sz,ret,buf_sz=0;
+    int *index_sizes,*index_disp,count,index_total_size=0;
+    char *index_files, *index_streams;
+    void *index_stream;
+    MPI_Comm hostdir_comm,hostdir_zeros_comm;
+    // Group index is the color
+    plfs_debug("XXXACXXX - mpi_adio/ad_plfs/ad_plfs_open::%s: call to adplfs_rank_to_group_index\n", __FUNCTION__);
+    group_index = adplfs_rank_to_group_index(rank,ranks_per_comm,extra_rank);
+    // Split the world communicator
+    MPI_Comm_split(fd->comm,group_index,rank,&hostdir_comm);
+    MPI_Comm_size(hostdir_comm,&hc_sz);
+    // Grab our rank within the hostdir communicator
+    MPI_Comm_rank (hostdir_comm, &new_rank);
+    // Get a color for a communicator between all rank 0's in a hostdir comm
+    if(!new_rank) {
+        color = 1;
+    } else {
+        color = MPI_UNDEFINED;
+    }
+    // The split for hostdir zeros comm
+    MPI_Comm_split(fd->comm,color,rank,&hostdir_zeros_comm);
+    // Hostdir zeros
+    if(!new_rank) {
+        plfs_debug("XXXACXXX - mpi_adio/ad_plfs/ad_plfs_open::%s: new_rank == %d\n", __FUNCTION__, new_rank);
+        char *subdir;
+        // Broadcast the bitmap to the leaders
+        BITMAP_PRINT;
+        plfs_debug("XXXACXXX - mpi_adio/ad_plfs/ad_plfs_open::%s: call to adplfs_bcast_bitmap\n", __FUNCTION__);
+        adplfs_bcast_bitmap(hostdir_zeros_comm,rank);
+        BITMAP_PRINT;
+        // Convert my group index into the dir I should read
+        // could be a Metalink
+        plfs_debug("XXXACXXX - mpi_adio/ad_plfs/ad_plfs_open::%s: call to adplfs_bitmap_to_dirname\n", __FUNCTION__);
+        subdir= adplfs_bitmap_to_dirname(bitmap,group_index,filename,0,np);
+
+        // Hostdir zero reads the hostdir and converts into a list
+        plfs_debug("XXXACXXX - mpi_adio/ad_plfs/ad_plfs_open::%s: call to container_hostdir_zero_rddir\n", __FUNCTION__);
+        container_hostdir_zero_rddir((void **)&index_files,subdir,rank,
+                                      pmount, pback, &buf_sz);
+        plfs_debug("XXXACXXX - mpi_adio/ad_plfs/ad_plfs_open::%s: call to check_stream\n", __FUNCTION__);
+        check_stream(buf_sz,rank);
+        free(subdir);
+    }
+
+    // Send the size of the hostdir file list
+    MPIBCAST(&buf_sz,1,MPI_INT,0,hostdir_comm);
+    // Get space for the hostdir file list
+    if(new_rank) {
+        index_files=malloc(sizeof(MPI_CHAR)*buf_sz);
+        malloc_check(index_files,rank);
+    }
+    // Get the hostdir file list
+    MPIBCAST(index_files,buf_sz,MPI_CHAR,0,hostdir_comm);
+
+    /*
+     * Take the hostdir file list and convert to an index stream.
+     * this is where we read the index files (done in parallel by
+     * hostdir and rank).  the first entry of index_files contains
+     * the physical path to the hostdir to read...
+     */
+    plfs_debug("XXXACXXX - mpi_adio/ad_plfs/ad_plfs_open::%s: call to container_parindex_read\n", __FUNCTION__);
+    container_parindex_read(new_rank,hc_sz,index_files,&index_stream,
+                              filename,&buf_sz);
+    check_stream(buf_sz,rank);
+    free(index_files);
+    if(!new_rank) {
+        index_sizes=malloc(sizeof(int)*hc_sz);
+        malloc_check((void *)index_sizes,rank);
+    }
+    // Make sure hostdir rank 0 knows how much index data to expect
+    MPIGATHER(&buf_sz,1,MPI_INT,index_sizes,1,
+              MPI_INT,0,hostdir_comm);
+    // Set up displacements
+    if(!new_rank) {
+        index_disp=malloc(sizeof(int)*hc_sz);
+        malloc_check((void *)index_disp,rank);
+        for(count=0; count<hc_sz; count++) {
+            // Displacements
+            index_disp[count]=index_total_size;
+            index_total_size+=index_sizes[count];
+        }
+        // Space for the index streams from my hostdir comm
+        index_streams=malloc(sizeof(char)*index_total_size);
+        malloc_check(index_stream,rank);
+    }
+    // Gather the index streams from your hostdir
+    MPIGATHERV(index_stream,buf_sz,MPI_CHAR,index_streams,index_sizes,
+               index_disp,MPI_CHAR,0,hostdir_comm);
+    void *hostdir_index_stream;
+    int hostdir_index_sz;
+    // No longer need this information
+    free(index_stream);
+    // Hostdir leader
+    if(!new_rank) {
+        /*
+         * the rank 0 for this hostdir merges the result of the
+         * reads into a single index for this hostdir.
+         */
+        plfs_debug("XXXACXXX - mpi_adio/ad_plfs/ad_plfs_open::%s: call to container_parindexread_merge\n", __FUNCTION__);
+        hostdir_index_sz=container_parindexread_merge(filename,index_streams,
+                         index_sizes,hc_sz,&hostdir_index_stream);
+        free(index_disp);
+        free(index_sizes);
+    }
+    int hzc_size,global_index_sz;
+    // Hostdir leader pass information to the other leaders
+    if(!new_rank) {
+        index_total_size=0;
+        MPI_Comm_size(hostdir_zeros_comm,&hzc_size);
+        // Store the sizes of all the leaders index streams
+        index_sizes=malloc(sizeof(int)*hzc_size);
+        malloc_check((void *)index_sizes,rank);
+        // Grab these sizes
+        MPIALLGATHER(&hostdir_index_sz,1,MPI_INT,index_sizes,
+                     1,MPI_INT,hostdir_zeros_comm);
+        // Set up for GatherV
+        index_disp=malloc(sizeof(int)*hzc_size);
+        malloc_check((void *) index_disp,rank);
+        for(count=0; count<hzc_size; count++) {
+            // Displacements
+            index_disp[count]=index_total_size;
+            index_total_size+=index_sizes[count];
+        }
+        index_streams=malloc(sizeof(char)*index_total_size);
+        malloc_check(index_streams,rank);
+        // Receive the index streams from all hotdir zeros
+        MPIALLGATHERV(hostdir_index_stream,hostdir_index_sz,MPI_CHAR,
+                      index_streams,index_sizes,index_disp,MPI_CHAR,
+                      hostdir_zeros_comm);
+        // free send buffers
+        free(hostdir_index_stream);
+        free(index_disp);
+        // Merge these streams into a single global index
+        plfs_debug("XXXACXXX - mpi_adio/ad_plfs/ad_plfs_open::%s: call to container_parindexread_merge\n", __FUNCTION__);
+        global_index_sz=container_parindexread_merge(filename,index_streams,
+                                                index_sizes,hzc_size,
+                                                global_index);
+        check_stream(global_index_sz,rank);
+        // Free all of our malloced structures
+        free(index_streams);
+        free(index_sizes);
+    }
+
+    /*
+     * now each hostdir leader has a complete global index of the file.
+     * each hostdir leader can now send the global index to its group
+     * members so that everyone has it...
+     */
+
+    // Get the size of the global index
+    MPIBCAST(&global_index_sz,1,MPI_INT,0,hostdir_comm);
+    // Malloc space if we are not hostdir leaders
+    if(new_rank) {
+        *global_index=malloc(sizeof(char)*global_index_sz);
+    }
+    malloc_check(global_index,rank);
+    //Hostdir leaders broadcast the global index
+    MPIBCAST(*global_index,global_index_sz,MPI_CHAR,0,hostdir_comm);
+    // Don't forget to  call Comm free  on created comms before MPI_Finalize
+    // or you will encounter problems
+    MPI_Comm_free(&hostdir_comm);
+    if(!new_rank) {
+        MPI_Comm_free(&hostdir_zeros_comm);
+    }
+}
+
+// Simple print function
+void adplfs_host_list_print(int line, Bitmap *bitmap)
+{
+    int count;
+    plfs_debug("printing hostdir bitmap from %d:\n", line);
+    for(count=0; count<BIT_ARRAY_LENGTH; count++) {
+        if(adplfs_bitIsSet(count,bitmap)) {
+            plfs_debug("Hostdir at position %d\n",count);
+        }
+    }
+}
+
+// Calculates the number of ranks per communication group
+// Split comm makes this many subgroups
+int adplfs_ranks_per_comm_calc(int np,int num_host_dir)
+{
+    return (num_host_dir>0?np/num_host_dir:0);
+}
+
+// Get the amount of left over ranks, our values
+// are not going to divide evenly
+int adplfs_extra_rank_calc(int np,int num_host_dir)
+{
+    return  np%num_host_dir;
+}
+
+// Using the rank get the index/color that determines the
+// subcommunication group that we belong in
+int adplfs_rank_to_group_index(int rank,int ranks_per_comm,int extra_rank)
+{
+    int ret;
+    if (rank < (extra_rank*(ranks_per_comm+1))) {
+        ret = rank / (ranks_per_comm+1);
+    } else {
+        ret = (rank - extra_rank)/ranks_per_comm;
+    }
+    return ret;
+}
+
+
+char *adplfs_count_to_hostdir(Bitmap *bitmap,int stop_point,int *count,
+                       int *hostdir_found, char *filename,char *target,
+                       int first)
+{
+    char hostdir_num[16];
+    plfs_debug("Searching bitmap from %d to %d\n",*count,stop_point);
+    while((*hostdir_found)<stop_point && *count < BIT_ARRAY_LENGTH) {
+        if(adplfs_bitIsSet(*count,bitmap)) {
+            (*hostdir_found)++;
+        }
+        (*count)++;
+    }
+    if(!first) {
+        strcpy(filename,target);
+    }
+    if(first) {
+        strcat(filename,target);
+    }
+    strcat(filename,"/");
+    strcat(filename,HOSTDIRPREFIX);
+    sprintf(hostdir_num,"%d",(*count)-1);
+    strcat(filename,hostdir_num);
+    return filename;
+}
+
+char *adplfs_bitmap_to_dirname(Bitmap *bitmap,int group_index,
+                        char *target,int mult,int np)
+{
+    char *path;
+    int count = 0;
+    int hostdir_found=0;
+    BITMAP_PRINT;
+    if(mult==0) {
+        path=malloc(sizeof(char)*4096);
+        path=adplfs_count_to_hostdir(bitmap,group_index+1,&count,
+                              &hostdir_found,path,target,0);
+    } else {
+        path=malloc(sizeof(char)*(mult*4096));
+        int dirs=0;
+        while(dirs<mult) {
+            int stop_point;
+            stop_point=(group_index+1)+(dirs*np);
+            path = adplfs_count_to_hostdir(bitmap,stop_point,&count,
+                                    &hostdir_found,path,target,count);
+            strcat(path,"|");
+            dirs++;
+        }
+    }
+    BITMAP_PRINT;
+    plfs_debug("%s returning %s (mult %d)\n", __FUNCTION__, path,mult);
+    return path;
+}
+
--- openmpi/ompi/mca/io/romio/romio/adio/ad_plfs/ad_plfs_read.c	1969-12-31 17:00:00.000000000 -0700
+++ openmpi-plfs-patch/ompi/mca/io/romio/romio/adio/ad_plfs/ad_plfs_read.c	2014-10-15 11:15:40.502467000 -0600
@@ -0,0 +1,77 @@
+/* -*- Mode: C; c-basic-offset:4 ; -*- */
+/*
+ *   $Id: ad_plfs_read.c,v 1.1 2010/11/29 19:59:01 adamm Exp $
+ *
+ *   Copyright (C) 1997 University of Chicago.
+ *   See COPYRIGHT notice in top-level directory.
+ */
+
+#include "adio.h"
+#include "adio_extern.h"
+#include "ad_plfs.h"
+
+#ifdef ROMIO_CRAY
+#include "../ad_cray/ad_cray.h"
+#endif /* ROMIO_CRAY */
+
+void ADIOI_PLFS_ReadContig(ADIO_File fd, void *buf, int count,
+                           MPI_Datatype datatype, int file_ptr_type,
+                           ADIO_Offset offset, ADIO_Status *status,
+                           int *error_code)
+{
+    plfs_error_t err = PLFS_TBD;
+    int datatype_size, rank;
+    ssize_t bytes_read;
+    ADIO_Offset len;
+    ADIO_Offset myoff;
+    static char myname[] = "ADIOI_PLFS_READCONTIG";
+#ifdef ROMIO_CRAY
+MPIIO_TIMER_START(RSYSIO);
+#endif /* ROMIO_CRAY */
+    MPI_Type_size(datatype, &datatype_size);
+    len = (ADIO_Offset)datatype_size * (ADIO_Offset)count;
+    MPI_Comm_rank( fd->comm, &rank );
+    // for the romio/test/large_file we always get an offset of 0
+    // maybe we need to increment fd->fp_ind ourselves?
+    if (file_ptr_type == ADIO_EXPLICIT_OFFSET) {
+        myoff = offset;
+    } else {
+        myoff = fd->fp_ind;
+    }
+    if (file_ptr_type == ADIO_INDIVIDUAL) {
+        myoff = fd->fp_ind;
+    }
+    plfs_debug( "%s: offset %ld len %ld rank %d\n",
+                myname, (long)myoff, (long)len, rank );
+    if ((fd->access_mode != ADIO_RDONLY) && fd->fs_ptr) {
+        // calls plfs_sync + barrier to ensure all ranks flush in-memory
+        // index before any rank calling plfs_read.
+        // need not do this for read-only file.
+        plfs_sync( fd->fs_ptr);
+        //we can't barrier here, MPI_File_read_at calls this function
+        //and is non-collective (we've seen a run hang where rank 0
+        //enters this function but rank 1 does not for example)
+        //plfs_barrier(fd->comm,rank);
+    }
+    plfs_debug("XXXACXXX - mpi_adio/ad_plfs/ad_plfs_read::%s: call to plfs_read\n", myname);
+    err = plfs_read( fd->fs_ptr, buf, len, myoff, &bytes_read );
+#ifdef HAVE_STATUS_SET_BYTES
+    if (err == PLFS_SUCCESS ) {
+        MPIR_Status_set_bytes(status, datatype, (int)bytes_read);
+    }
+#endif
+    if (err != PLFS_SUCCESS ) {
+        *error_code = MPIO_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE,
+                                           myname, __LINE__, MPI_ERR_IO,
+                                           "**io",
+                                           "**io %s", strplfserr(err));
+    } else {
+        if (file_ptr_type == ADIO_INDIVIDUAL) {
+            fd->fp_ind += bytes_read;
+        }
+        *error_code = MPI_SUCCESS;
+    }
+#ifdef ROMIO_CRAY
+MPIIO_TIMER_END(RSYSIO);
+#endif /* ROMIO_CRAY */
+}
--- openmpi/ompi/mca/io/romio/romio/adio/ad_plfs/ad_plfs_resize.c	1969-12-31 17:00:00.000000000 -0700
+++ openmpi-plfs-patch/ompi/mca/io/romio/romio/adio/ad_plfs/ad_plfs_resize.c	2014-10-15 11:15:40.503468000 -0600
@@ -0,0 +1,107 @@
+/* -*- Mode: C; c-basic-offset:4 ; -*- */
+/*
+ *   $Id: ad_plfs_resize.c,v 1.1 2010/11/29 19:59:01 adamm Exp $
+ *
+ *   Copyright (C) 1997 University of Chicago.
+ *   See COPYRIGHT notice in top-level directory.
+ */
+
+#include "ad_plfs.h"
+
+extern void reduce_meta(ADIO_File afd, Plfs_fd *fd, const char *filename,
+                        Plfs_close_opt *close_opt, int rank);
+
+void ADIOI_PLFS_Resize(ADIO_File fd, ADIO_Offset size, int *error_code)
+{
+    int err, rank, procs, amode, perm;
+    plfs_error_t plfs_err;
+    int num_open_handles;
+    size_t bytes_written=0, total_bytes=0, flatten=0;
+    Plfs_open_opt open_opt;
+    Plfs_close_opt close_opt;
+    int file_is_open=1, do_close_reopen=0;
+    uid_t uid = geteuid();
+    static char myname[] = "ADIOI_PLFS_RESIZE";
+    plfs_debug( "%s: begin\n", myname );
+    /* don't permit resizing/truncating a read-only file */
+    if (fd->access_mode == ADIO_RDONLY) {
+        err = EPERM;
+        *error_code = MPIO_Err_create_code(err, MPIR_ERR_RECOVERABLE,
+                                           myname, __LINE__, MPI_ERR_READ_ONLY,
+                                           "**io",
+                                           "**io %s", strerror(err));
+        plfs_debug( "%s: cannot resize/truncate a read-only file.\n", myname );
+        return;
+    }
+    MPI_Comm_rank(fd->comm, &rank);
+    MPI_Comm_size( fd->comm, &procs);
+    // in container mode, we might need to close then reopen to ensure
+    // the truncate works correctly.  flat file doesn't need this though
+    if (plfs_get_filetype(fd->filename) == CONTAINER) {
+        /* do close+reopen when user ever wrote something. */
+        plfs_query(fd->fs_ptr, NULL, NULL, &bytes_written, NULL);
+        MPI_Allreduce(&bytes_written, &total_bytes, 1, MPI_LONG_LONG,
+                      MPI_SUM, fd->comm);
+        if (bytes_written) {
+            do_close_reopen = 1;
+        }
+        if (do_close_reopen) {
+            plfs_debug( "%s: do close+reopen for truncate.\n", myname );
+            close_opt.pinter=PLFS_MPIIO;
+            close_opt.num_procs = procs;
+            reduce_meta(fd, fd->fs_ptr, fd->filename, &close_opt, rank);
+            amode = ad_plfs_amode(fd->access_mode);
+            plfs_close(fd->fs_ptr, rank, uid, amode, &close_opt, &num_open_handles);
+            file_is_open = 0;
+            fd->fs_ptr = NULL;
+            plfs_barrier(fd->comm,rank);
+        }
+    }
+    /* do the truncate */
+    if (rank == fd->hints->ranklist[0]) {
+        // running the silverton test code we are seeing that
+        // rank 1 has already opened the file and then rank 0
+        // gets the Resize call and doesn't see that the file is open
+        // then we 0 calls this with file_is_open==0, plfs_trunc in
+        // container mode does unlink of droppings thereby destroying
+        // rank 1's open files.  Thus, let's always do open_file==1 here
+        // since we can't tell for sure that someone else doesn't have it
+        // open.  then plfs_trunc internal will only truncate droppings and
+        // not delete
+        file_is_open=1;
+        plfs_err = plfs_trunc(fd->fs_ptr, fd->filename, size, file_is_open);
+        err = -(plfs_error_to_errno(plfs_err));
+    }
+    // we want to barrier so that no-one leaves until we are done truncating
+    // we are relying on MPI_Bcast to do an effective barrier for us
+    MPI_Bcast(&err, 1, MPI_INT, fd->hints->ranklist[0], fd->comm);
+    if (err < 0) {
+        *error_code = MPIO_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE,
+                                           myname, __LINE__, MPI_ERR_IO,
+                                           "**io",
+                                           "**io %s", strerror(-err));
+    } else {
+        *error_code = MPI_SUCCESS;
+    }
+    /* do re-open if file was closed */
+    if (do_close_reopen) {
+        flatten = ad_plfs_hints (fd , rank, "plfs_flatten_close");
+        open_opt.pinter = PLFS_MPIIO;
+        open_opt.index_stream = NULL;
+        open_opt.reopen = 1;
+        open_opt.buffer_index = 0;
+        if (flatten != -1) {
+            open_opt.buffer_index = flatten;
+        }
+        perm = adplfs_getPerm(fd);
+        plfs_err = plfs_open( (Plfs_fd **)&(fd->fs_ptr), fd->filename, amode, rank,
+                              perm, &open_opt);
+        if ((plfs_err != PLFS_SUCCESS) && (*error_code == MPI_SUCCESS)) {
+            *error_code = MPIO_Err_create_code(MPI_SUCCESS,
+                                               MPIR_ERR_RECOVERABLE,
+                                               myname, __LINE__, MPI_ERR_IO,
+                                               "**io",
+                                               "**io %s", strplfserr(plfs_err));
+        }
+    }
+}
--- openmpi/ompi/mca/io/romio/romio/adio/ad_plfs/ad_plfs_write.c	1969-12-31 17:00:00.000000000 -0700
+++ openmpi-plfs-patch/ompi/mca/io/romio/romio/adio/ad_plfs/ad_plfs_write.c	2014-10-15 11:15:40.504469000 -0600
@@ -0,0 +1,71 @@
+/* -*- Mode: C; c-basic-offset:4 ; -*- */
+/*
+ *   $Id: ad_plfs_write.c,v 1.1 2010/11/29 19:59:01 adamm Exp $
+ *
+ *   Copyright (C) 1997 University of Chicago.
+ *   See COPYRIGHT notice in top-level directory.
+ */
+
+#include "ad_plfs.h"
+#include "adio_extern.h"
+
+#ifdef ROMIO_CRAY
+#include "../ad_cray/ad_cray.h"
+#endif /* ROMIO_CRAY */
+
+void ADIOI_PLFS_WriteContig(ADIO_File fd, void *buf, int count,
+                            MPI_Datatype datatype, int file_ptr_type,
+                            ADIO_Offset offset, ADIO_Status *status,
+                            int *error_code)
+{
+    /* --BEGIN CRAY MODIFICATION-- */
+    plfs_error_t err = PLFS_TBD;
+    int datatype_size, rank;
+    ssize_t bytes_written;
+    ADIO_Offset len;
+    /* --END CRAY MODIFICATION-- */
+    ADIO_Offset myoff;
+    static char myname[] = "ADIOI_PLFS_WRITECONTIG";
+#ifdef ROMIO_CRAY
+MPIIO_TIMER_START(WSYSIO);
+#endif /* ROMIO_CRAY */
+    MPI_Type_size(datatype, &datatype_size);
+    /* --BEGIN CRAY MODIFICATION-- */
+    len = (ADIO_Offset)datatype_size * (ADIO_Offset)count;
+    /* --END CRAY MODIFICATION-- */
+    MPI_Comm_rank( fd->comm, &rank );
+    // for the romio/test/large_file we always get an offset of 0
+    // maybe we need to increment fd->fp_ind ourselves?
+    if (file_ptr_type == ADIO_EXPLICIT_OFFSET) {
+        myoff = offset;
+    } else {
+        myoff = fd->fp_ind;
+    }
+    if (file_ptr_type == ADIO_INDIVIDUAL) {
+        myoff = fd->fp_ind;
+    }
+    plfs_debug( "%s: offset %ld len %ld rank %d\n",
+                myname, (long)myoff, (long)len, rank );
+    plfs_debug("XXXACXXX - mpi_adio/ad_plfs/ad_plfs_read::%s: call to plfs_write\n", myname);
+    err = plfs_write( fd->fs_ptr, buf, len, myoff, rank, &bytes_written );
+#ifdef HAVE_STATUS_SET_BYTES
+    if (err == PLFS_SUCCESS ) {
+        MPIR_Status_set_bytes(status, datatype, (int)bytes_written);
+    }
+#endif
+    if (err != PLFS_SUCCESS ) {
+        *error_code = MPIO_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE,
+                                           myname, __LINE__, MPI_ERR_IO,
+                                           "**io",
+                                           "**io %s", strplfserr(err));
+    } else {
+        if (file_ptr_type == ADIO_INDIVIDUAL) {
+            fd->fp_ind += bytes_written;
+        }
+        *error_code = MPI_SUCCESS;
+    }
+#ifdef ROMIO_CRAY
+MPIIO_TIMER_END(WSYSIO);
+#endif /* ROMIO_CRAY */
+}
+
--- openmpi/ompi/mca/io/romio/romio/adio/ad_plfs/Makefile.am	1969-12-31 17:00:00.000000000 -0700
+++ openmpi-plfs-patch/ompi/mca/io/romio/romio/adio/ad_plfs/Makefile.am	2014-10-15 11:15:40.515477000 -0600
@@ -0,0 +1,38 @@
+#
+# Copyright (c) 2004-2005 The Trustees of Indiana University and Indiana
+#                         University Research and Technology
+#                         Corporation.  All rights reserved.
+# Copyright (c) 2004-2005 The University of Tennessee and The University
+#                         of Tennessee Research Foundation.  All rights
+#                         reserved.
+# Copyright (c) 2004-2005 High Performance Computing Center Stuttgart, 
+#                         University of Stuttgart.  All rights reserved.
+# Copyright (c) 2004-2005 The Regents of the University of California.
+#                         All rights reserved.
+# Copyright (c) 2008      Cisco Systems, Inc.  All rights reserved.
+# $COPYRIGHT$
+# 
+# Additional copyrights may follow
+# 
+# $HEADER$
+#
+
+include $(top_srcdir)/Makefile.options
+
+#libadio_plfs_la_LDFLAGS = $(AD_PLFS_LDFLAGS)
+#libadio_plfs_la_CFLAGS = $(AD_PLFS_CFLAGS)
+
+noinst_LTLIBRARIES = libadio_plfs.la
+libadio_plfs_la_SOURCES = \
+	ad_plfs.c \
+	ad_plfs.h \
+	ad_plfs_close.c \
+	ad_plfs_delete.c \
+	ad_plfs_fcntl.c \
+	ad_plfs_features.c \
+	ad_plfs_flush.c \
+	ad_plfs_open.c \
+	ad_plfs_read.c \
+	ad_plfs_resize.c \
+	ad_plfs_hints.c \
+	ad_plfs_write.c 
